{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python threedconf.py\n",
    "\n",
    "Description: This file contains the proposed basic architecture to generate 3-D\n",
    "            conformers. The code is imported from https://github.com/divelab/DIG\n",
    "            and edited in order to include Sphernet as embeddings layer.\n",
    "\n",
    "Author: Abanoub Abdelmalak\n",
    "\n",
    "Date Created: May 1, 2023\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Linear, Embedding\n",
    "from torch_geometric.nn.inits import glorot_orthogonal\n",
    "from torch_geometric.nn import radius_graph\n",
    "from torch_scatter import scatter\n",
    "from math import sqrt\n",
    "\n",
    "from dig.threedgraph.utils import xyz_to_dat\n",
    "from dig.threedgraph.method.spherenet.features import dist_emb, angle_emb, torsion_emb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "class emb(torch.nn.Module):\n",
    "    def __init__(self, num_spherical, num_radial, cutoff, envelope_exponent):\n",
    "        super(emb, self).__init__()\n",
    "        self.dist_emb = dist_emb(num_radial, cutoff, envelope_exponent)\n",
    "        self.angle_emb = angle_emb(num_spherical, num_radial, cutoff, envelope_exponent)\n",
    "        self.torsion_emb = torsion_emb(num_spherical, num_radial, cutoff, envelope_exponent)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.dist_emb.reset_parameters()\n",
    "\n",
    "    def forward(self, dist, angle, torsion, idx_kj):\n",
    "        dist_emb = self.dist_emb(dist)\n",
    "        angle_emb = self.angle_emb(dist, angle, idx_kj)\n",
    "        torsion_emb = self.torsion_emb(dist, angle, torsion, idx_kj)\n",
    "        return dist_emb, angle_emb, torsion_emb\n",
    "\n",
    "class ResidualLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, act=swish):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.act = act\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin1.weight, scale=2.0)\n",
    "        self.lin1.bias.data.fill_(0)\n",
    "        glorot_orthogonal(self.lin2.weight, scale=2.0)\n",
    "        self.lin2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.act(self.lin2(self.act(self.lin1(x))))\n",
    "\n",
    "\n",
    "class init(torch.nn.Module):\n",
    "    def __init__(self, num_radial, hidden_channels, act=swish, use_node_features=True):\n",
    "        super(init, self).__init__()\n",
    "        self.act = act\n",
    "        self.use_node_features = use_node_features\n",
    "        if self.use_node_features:\n",
    "            self.emb = Embedding(95, hidden_channels)\n",
    "        else: # option to use no node features and a learned embedding vector for each node instead\n",
    "            self.node_embedding = nn.Parameter(torch.empty((hidden_channels,)))\n",
    "            nn.init.normal_(self.node_embedding)\n",
    "        self.lin_rbf_0 = Linear(num_radial, hidden_channels)\n",
    "        self.lin = Linear(3 * hidden_channels, hidden_channels)\n",
    "        self.lin_rbf_1 = nn.Linear(num_radial, hidden_channels, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.use_node_features:\n",
    "            self.emb.weight.data.uniform_(-sqrt(3), sqrt(3))\n",
    "        self.lin_rbf_0.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "        glorot_orthogonal(self.lin_rbf_1.weight, scale=2.0)\n",
    "\n",
    "    def forward(self, x, emb, i, j):\n",
    "        rbf,_,_ = emb\n",
    "        if self.use_node_features:\n",
    "            x = self.emb(x)\n",
    "        else:\n",
    "            x = self.node_embedding[None, :].expand(x.shape[0], -1)\n",
    "        rbf0 = self.act(self.lin_rbf_0(rbf))\n",
    "        e1 = self.act(self.lin(torch.cat([x[i], x[j], rbf0], dim=-1)))\n",
    "        e2 = self.lin_rbf_1(rbf) * e1\n",
    "\n",
    "        return e1, e2\n",
    "\n",
    "\n",
    "class update_e(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, int_emb_size, basis_emb_size_dist, basis_emb_size_angle, basis_emb_size_torsion, num_spherical, num_radial,\n",
    "        num_before_skip, num_after_skip, act=swish):\n",
    "        super(update_e, self).__init__()\n",
    "        self.act = act\n",
    "        self.lin_rbf1 = nn.Linear(num_radial, basis_emb_size_dist, bias=False)\n",
    "        self.lin_rbf2 = nn.Linear(basis_emb_size_dist, hidden_channels, bias=False)\n",
    "        self.lin_sbf1 = nn.Linear(num_spherical * num_radial, basis_emb_size_angle, bias=False)\n",
    "        self.lin_sbf2 = nn.Linear(basis_emb_size_angle, int_emb_size, bias=False)\n",
    "        self.lin_t1 = nn.Linear(num_spherical * num_spherical * num_radial, basis_emb_size_torsion, bias=False)\n",
    "        self.lin_t2 = nn.Linear(basis_emb_size_torsion, int_emb_size, bias=False)\n",
    "        self.lin_rbf = nn.Linear(num_radial, hidden_channels, bias=False)\n",
    "\n",
    "        self.lin_kj = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin_ji = nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.lin_down = nn.Linear(hidden_channels, int_emb_size, bias=False)\n",
    "        self.lin_up = nn.Linear(int_emb_size, hidden_channels, bias=False)\n",
    "\n",
    "        self.layers_before_skip = torch.nn.ModuleList([\n",
    "            ResidualLayer(hidden_channels, act)\n",
    "            for _ in range(num_before_skip)\n",
    "        ])\n",
    "        self.lin = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.layers_after_skip = torch.nn.ModuleList([\n",
    "            ResidualLayer(hidden_channels, act)\n",
    "            for _ in range(num_after_skip)\n",
    "        ])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin_rbf1.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_rbf2.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_sbf1.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_sbf2.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_t1.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_t2.weight, scale=2.0)\n",
    "\n",
    "        glorot_orthogonal(self.lin_kj.weight, scale=2.0)\n",
    "        self.lin_kj.bias.data.fill_(0)\n",
    "        glorot_orthogonal(self.lin_ji.weight, scale=2.0)\n",
    "        self.lin_ji.bias.data.fill_(0)\n",
    "\n",
    "        glorot_orthogonal(self.lin_down.weight, scale=2.0)\n",
    "        glorot_orthogonal(self.lin_up.weight, scale=2.0)\n",
    "\n",
    "        for res_layer in self.layers_before_skip:\n",
    "            res_layer.reset_parameters()\n",
    "        glorot_orthogonal(self.lin.weight, scale=2.0)\n",
    "        self.lin.bias.data.fill_(0)\n",
    "        for res_layer in self.layers_after_skip:\n",
    "            res_layer.reset_parameters()\n",
    "\n",
    "        glorot_orthogonal(self.lin_rbf.weight, scale=2.0)\n",
    "\n",
    "    def forward(self, x, emb, idx_kj, idx_ji):\n",
    "        rbf0, sbf, t = emb\n",
    "        x1,_ = x\n",
    "\n",
    "        x_ji = self.act(self.lin_ji(x1))\n",
    "        x_kj = self.act(self.lin_kj(x1))\n",
    "\n",
    "        rbf = self.lin_rbf1(rbf0)\n",
    "        rbf = self.lin_rbf2(rbf)\n",
    "        x_kj = x_kj * rbf\n",
    "\n",
    "        x_kj = self.act(self.lin_down(x_kj))\n",
    "\n",
    "        sbf = self.lin_sbf1(sbf)\n",
    "        sbf = self.lin_sbf2(sbf)\n",
    "        x_kj = x_kj[idx_kj] * sbf\n",
    "\n",
    "        t = self.lin_t1(t)\n",
    "        t = self.lin_t2(t)\n",
    "        x_kj = x_kj * t\n",
    "\n",
    "        x_kj = scatter(x_kj, idx_ji, dim=0, dim_size=x1.size(0))\n",
    "        x_kj = self.act(self.lin_up(x_kj))\n",
    "\n",
    "        e1 = x_ji + x_kj\n",
    "        for layer in self.layers_before_skip:\n",
    "            e1 = layer(e1)\n",
    "        e1 = self.act(self.lin(e1)) + x1\n",
    "        for layer in self.layers_after_skip:\n",
    "            e1 = layer(e1)\n",
    "        e2 = self.lin_rbf(rbf0) * e1\n",
    "\n",
    "        return e1, e2\n",
    "\n",
    "\n",
    "class update_v(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_emb_channels, out_channels, num_output_layers, act, output_init):\n",
    "        super(update_v, self).__init__()\n",
    "        self.act = act\n",
    "        self.output_init = output_init\n",
    "\n",
    "        self.lin_up = nn.Linear(hidden_channels, out_emb_channels, bias=True)\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        for _ in range(num_output_layers):\n",
    "            self.lins.append(nn.Linear(out_emb_channels, out_emb_channels))\n",
    "        self.lin = nn.Linear(out_emb_channels, out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot_orthogonal(self.lin_up.weight, scale=2.0)\n",
    "        for lin in self.lins:\n",
    "            glorot_orthogonal(lin.weight, scale=2.0)\n",
    "            lin.bias.data.fill_(0)\n",
    "        if self.output_init == 'zeros':\n",
    "            self.lin.weight.data.fill_(0)\n",
    "        if self.output_init == 'GlorotOrthogonal':\n",
    "            glorot_orthogonal(self.lin.weight, scale=2.0)\n",
    "\n",
    "    def forward(self, e, i):\n",
    "        _, e2 = e\n",
    "        v = scatter(e2, i, dim=0)\n",
    "        v = self.lin_up(v)\n",
    "        for lin in self.lins:\n",
    "            v = self.act(lin(v))\n",
    "        v = self.lin(v)\n",
    "        return v\n",
    "\n",
    "\n",
    "class update_u(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(update_u, self).__init__()\n",
    "\n",
    "    def forward(self, u, v, batch):\n",
    "        u += scatter(v, batch, dim=0)\n",
    "        return u\n",
    "\n",
    "\n",
    "class SphereNet(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "         The spherical message passing neural network SphereNet from the `\"Spherical Message Passing for 3D Molecular Graphs\" <https://openreview.net/forum?id=givsRXsOt9r>`_ paper.\n",
    "        \n",
    "        Args:\n",
    "            energy_and_force (bool, optional): If set to :obj:`True`, will predict energy and take the negative of the derivative of the energy with respect to the atomic positions as predicted forces. (default: :obj:`False`)\n",
    "            cutoff (float, optional): Cutoff distance for interatomic interactions. (default: :obj:`5.0`)\n",
    "            num_layers (int, optional): Number of building blocks. (default: :obj:`4`)\n",
    "            hidden_channels (int, optional): Hidden embedding size. (default: :obj:`128`)\n",
    "            out_channels (int, optional): Size of each output sample. (default: :obj:`1`)\n",
    "            int_emb_size (int, optional): Embedding size used for interaction triplets. (default: :obj:`64`)\n",
    "            basis_emb_size_dist (int, optional): Embedding size used in the basis transformation of distance. (default: :obj:`8`)\n",
    "            basis_emb_size_angle (int, optional): Embedding size used in the basis transformation of angle. (default: :obj:`8`)\n",
    "            basis_emb_size_torsion (int, optional): Embedding size used in the basis transformation of torsion. (default: :obj:`8`)\n",
    "            out_emb_channels (int, optional): Embedding size used for atoms in the output block. (default: :obj:`256`)\n",
    "            num_spherical (int, optional): Number of spherical harmonics. (default: :obj:`7`)\n",
    "            num_radial (int, optional): Number of radial basis functions. (default: :obj:`6`)\n",
    "            envelop_exponent (int, optional): Shape of the smooth cutoff. (default: :obj:`5`)\n",
    "            num_before_skip (int, optional): Number of residual layers in the interaction blocks before the skip connection. (default: :obj:`1`)\n",
    "            num_after_skip (int, optional): Number of residual layers in the interaction blocks before the skip connection. (default: :obj:`2`)\n",
    "            num_output_layers (int, optional): Number of linear layers for the output blocks. (default: :obj:`3`)\n",
    "            act: (function, optional): The activation funtion. (default: :obj:`swish`)\n",
    "            output_init: (str, optional): The initialization fot the output. It could be :obj:`GlorotOrthogonal` and :obj:`zeros`. (default: :obj:`GlorotOrthogonal`)\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, energy_and_force=False, cutoff=5.0, num_layers=4,\n",
    "        hidden_channels=128, out_channels=1, int_emb_size=64,\n",
    "        basis_emb_size_dist=8, basis_emb_size_angle=8, basis_emb_size_torsion=8, out_emb_channels=256,\n",
    "        num_spherical=7, num_radial=6, envelope_exponent=5,\n",
    "        num_before_skip=1, num_after_skip=2, num_output_layers=3,\n",
    "        act=swish, output_init='GlorotOrthogonal', use_node_features=True):\n",
    "        super(SphereNet, self).__init__()\n",
    "\n",
    "        self.cutoff = cutoff\n",
    "        self.energy_and_force = energy_and_force\n",
    "\n",
    "        self.init_e = init(num_radial, hidden_channels, act, use_node_features=use_node_features)\n",
    "        self.init_v = update_v(hidden_channels, out_emb_channels, out_channels, num_output_layers, act, output_init)\n",
    "        self.init_u = update_u()\n",
    "        self.emb = emb(num_spherical, num_radial, self.cutoff, envelope_exponent)\n",
    "\n",
    "        self.update_vs = torch.nn.ModuleList([\n",
    "            update_v(hidden_channels, out_emb_channels, out_channels, num_output_layers, act, output_init) for _ in range(num_layers)])\n",
    "\n",
    "        self.update_es = torch.nn.ModuleList([\n",
    "            update_e(hidden_channels, int_emb_size, basis_emb_size_dist, basis_emb_size_angle, basis_emb_size_torsion, num_spherical, num_radial, num_before_skip, num_after_skip,act) for _ in range(num_layers)])\n",
    "\n",
    "        self.update_us = torch.nn.ModuleList([update_u() for _ in range(num_layers)])\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.init_e.reset_parameters()\n",
    "        self.init_v.reset_parameters()\n",
    "        self.emb.reset_parameters()\n",
    "        for update_e in self.update_es:\n",
    "            update_e.reset_parameters()\n",
    "        for update_v in self.update_vs:\n",
    "            update_v.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        z, pos, batch = batch_data.z, batch_data.pos, batch_data.batch\n",
    "        if self.energy_and_force:\n",
    "            pos.requires_grad_()\n",
    "        edge_index = radius_graph(pos, r=self.cutoff, batch=batch)\n",
    "        num_nodes=z.size(0)\n",
    "        dist, angle, torsion, i, j, idx_kj, idx_ji = xyz_to_dat(pos, edge_index, num_nodes, use_torsion=True)\n",
    "\n",
    "        emb = self.emb(dist, angle, torsion, idx_kj)\n",
    "        #print(batch_data.shape)\n",
    "        #Initialize edge, node, graph features\n",
    "        e = self.init_e(z, emb, i, j)\n",
    "        v = self.init_v(e, i)\n",
    "        u = self.init_u(torch.zeros_like(scatter(v, batch, dim=0)), v, batch) #scatter(v, batch, dim=0)\n",
    "\n",
    "        for update_e, update_v, update_u in zip(self.update_es, self.update_vs, self.update_us):\n",
    "            e = update_e(e, emb, idx_kj, idx_ji)\n",
    "            v = update_v(e, i)\n",
    "            u = update_u(u, v, batch) #u += scatter(v, batch, dim=0)\n",
    "\n",
    "        return u, v, e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class SphernetWithTransformer(SphereNet):\n",
    "    def __init__(self, energy_and_force=False, cutoff=5.0, num_layers=4, \n",
    "        hidden_channels=128, out_channels=1, int_emb_size=64, \n",
    "        basis_emb_size_dist=8, basis_emb_size_angle=8, basis_emb_size_torsion=8, out_emb_channels=256, \n",
    "        num_spherical=3, num_radial=6, envelope_exponent=5, \n",
    "        num_before_skip=1, num_after_skip=2, num_output_layers=3, use_node_features=True):\n",
    "        super(SphernetWithTransformer, self).__init__()\n",
    "\n",
    "        self.cutoff = cutoff\n",
    "        self.energy_and_force = energy_and_force\n",
    "\n",
    "        self.spherenet = SphereNet(energy_and_force=False, cutoff=5.0, num_layers=4, \n",
    "        hidden_channels=128, out_channels=1, int_emb_size=64, \n",
    "        basis_emb_size_dist=8, basis_emb_size_angle=8, basis_emb_size_torsion=8, out_emb_channels=256, \n",
    "        num_spherical=3, num_radial=6, envelope_exponent=5, \n",
    "        num_before_skip=1, num_after_skip=2, num_output_layers=3, use_node_features=True)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model=256, nhead=8)\n",
    "\n",
    "        self.linear_out = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        batch_data.to(device)\n",
    "        z, pos, batch = batch_data.z, batch_data.pos, batch_data.batch\n",
    "        \n",
    "        # Forward pass through the SphereNet\n",
    "        u, z_pred, _ = self.spherenet(batch_data)\n",
    "\n",
    "        # Encode the latent representation with the transformer\n",
    "        z_enc = self.transformer(z_pred.unsqueeze(1))\n",
    "\n",
    "        # Decode the latent representation with a linear layer\n",
    "        pos_pred = self.linear_out(z_enc)\n",
    "\n",
    "        return pos_pred, z_enc\n",
    "def loss_fn(pos, pos_pred):\n",
    "    \"\"\"\n",
    "    Computes the loss between the original positions and the predicted positions.\n",
    "\n",
    "    Args:\n",
    "        pos: The original positions.\n",
    "        pos_pred: The predicted positions.\n",
    "\n",
    "    Returns:\n",
    "        The loss between the original positions and the predicted positions.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = torch.pow(pos - pos_pred, 2).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "from torch_geometric.datasets import QM9\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "import torch_geometric.data.data\n",
    "from torch_geometric.data import Dataset, Data, Batch, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "#from torch.utils.data import DataLoader\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "from torch_geometric.data.collate import collate\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from rdkit.Chem.rdchem import HybridizationType\n",
    "from rdkit.Chem.rdchem import BondType as BT\n",
    "from rdkit import Chem\n",
    "from torch.nn.functional import one_hot\n",
    "from torch import scatter\n",
    "import random\n",
    "from rdkit.Chem.rdchem import ChiralType\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "#from dig.threedgraph.method import SphereNet\n",
    "from dig.threedgraph import method\n",
    "from dig.threedgraph.evaluation import ThreeDEvaluator\n",
    "from dig.threedgraph.method import run\n",
    "from qm9_dataset import qm9_data_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../../../others_approaches/conformation_generation/GeoMol/data/QM9/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is included!!\n"
     ]
    }
   ],
   "source": [
    "qm9_set = qm9_data_geom(root= path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "print(qm9_set[2].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target = 'boltzmann_weight' # mu, alpha, homo, lumo, gap, r2, zpve, U0, U, H, G, Cv\n",
    "#qm9_set.data.y = qm9_set.data[target]\n",
    "\n",
    "split_idx = qm9_set.get_idx_split(len(qm9_set.data.y)-2, train_size=300, valid_size=100, seed=42)\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = qm9_set[split_idx['train']], qm9_set[split_idx['valid']], qm9_set[split_idx['test']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "def evaluation(pred, target):\n",
    "    pred = pred.cpu().detach().numpy()\n",
    "    target = target.cpu().detach().numpy()\n",
    "    rmse = np.sqrt(np.mean((pred - target)**2))\n",
    "\n",
    "    return rmse\n",
    "def trainer(model, loss_func, train_dataset, valid_dataset, epochs=100):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        for batch in DataLoader(train_dataset, batch_size=4, shuffle=True):\n",
    "            batch = batch.to(device)\n",
    "            pred, z_pred = model(batch)\n",
    "            loss = loss_func(pred, batch.pos)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        with torch.no_grad():\n",
    "            pred, z_pred = model(valid_dataset.to(device))\n",
    "            loss = loss_func(pred, valid_dataset.pos.to(device))\n",
    "            rmse = evaluation(pred.cpu().detach().numpy(), valid_dataset.pos.cpu().detach().numpy())\n",
    "\n",
    "        print(f\"Epoch {epoch}: loss {loss.item():.4f}, RMSE {rmse:.4f}\")\n",
    "def generate_3d_positions(model, latent_representations):\n",
    "    pred_positions = []\n",
    "    for latent_representation in latent_representations:\n",
    "        z_pred = model.autoencoder(latent_representation)\n",
    "        pred_position = np.array(z_pred.cpu().detach().numpy())\n",
    "        pred_positions.append(pred_position)\n",
    "\n",
    "    return pred_positions\n",
    "\n",
    "def train(model, dataloader, optimizer, loss_fn, num_epochs):\n",
    "    \"\"\"\n",
    "    Trains the model on the given dataloader for the specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        dataloader: The dataloader to use for training.\n",
    "        optimizer: The optimizer to use for training.\n",
    "        loss_fn: The loss function to use for training.\n",
    "        num_epochs: The number of epochs to train for.\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dataloader:\n",
    "            batch.to(\"cuda:0\")\n",
    "            pos, pos_pred = model(batch)\n",
    "            loss = loss_fn(pos, pos_pred)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SphernetWithTransformer().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#loss_fn = loss_fn().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/DIG-Stable/lib/python3.8/site-packages/torch_geometric/data/storage.py:48\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DIG-Stable/lib/python3.8/site-packages/torch_geometric/data/storage.py:68\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mapping[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#train_dataset.to(device)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m trained_model \u001b[39m=\u001b[39m train(model, dataloader, optimizer, loss_fn, \u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[24], line 54\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, num_epochs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m     53\u001b[0m     batch\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m     tgt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(batch\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m], batch\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m     pos, pos_pred \u001b[39m=\u001b[39m model(batch, tgt)\n\u001b[1;32m     56\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pos, pos_pred)\n",
      "File \u001b[0;32m~/anaconda3/envs/DIG-Stable/lib/python3.8/site-packages/torch_geometric/data/data.py:345\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_store\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m    340\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    341\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m object was created by an older version of PyG. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this error occurred while loading an already existing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    343\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset, remove the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mprocessed/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directory in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    344\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mroot folder and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 345\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store, key)\n",
      "File \u001b[0;32m~/anaconda3/envs/DIG-Stable/lib/python3.8/site-packages/torch_geometric/data/storage.py:50\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#train_dataset.to(device)\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "trained_model = train(model, dataloader, optimizer, loss_fn, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, loss_func, train_dataset, valid_dataset, epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_func' is not defined"
     ]
    }
   ],
   "source": [
    "train(model, loss_func, train_dataset, valid_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qm9_data_geom[2].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SphereNet(energy_and_force=False, cutoff=5.0, num_layers=4, \n",
    "        hidden_channels=128, out_channels=1, int_emb_size=64, \n",
    "        basis_emb_size_dist=8, basis_emb_size_angle=8, basis_emb_size_torsion=8, out_emb_channels=256, \n",
    "        num_spherical=3, num_radial=6, envelope_exponent=5, \n",
    "        num_before_skip=1, num_after_skip=2, num_output_layers=3, use_node_features=True)\n",
    "loss_func = torch.nn.L1Loss()\n",
    "evaluation = ThreeDEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run3d = run()\n",
    "run3d.run(device, train_dataset, valid_dataset, test_dataset, \n",
    "        model, loss_func, evaluation, \n",
    "        epochs=2, batch_size=4, vt_batch_size=4, lr=0.0005, lr_decay_factor=0.5, lr_decay_step_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    print(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dig.threedgraph.dataset import QM93D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QM93D(root='../../../others_approaches/embeddings_nets/DIG-dig-stable/tutorials/KDD2022/dataset/')\n",
    "target = 'U0' # mu, alpha, homo, lumo, gap, r2, zpve, U0, U, H, G, Cv\n",
    "dataset.data.y = dataset.data[target]\n",
    "\n",
    "split_idx = dataset.get_idx_split(len(dataset.data.y), train_size=110000, valid_size=10000, seed=42)\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = dataset[split_idx['train']], dataset[split_idx['valid']], dataset[split_idx['test']]\n",
    "print('train, validaion, test:', len(train_dataset), len(valid_dataset), len(test_dataset))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SphereNet(energy_and_force=False, cutoff=5.0, num_layers=4, \n",
    "        hidden_channels=128, out_channels=1, int_emb_size=64, \n",
    "        basis_emb_size_dist=8, basis_emb_size_angle=8, basis_emb_size_torsion=8, out_emb_channels=256, \n",
    "        num_spherical=3, num_radial=6, envelope_exponent=5, \n",
    "        num_before_skip=1, num_after_skip=2, num_output_layers=3, use_node_features=True)\n",
    "loss_func = torch.nn.L1Loss()\n",
    "evaluation = ThreeDEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run3d = run()\n",
    "run3d.run(device, train_dataset, valid_dataset, test_dataset, \n",
    "        model, loss_func, evaluation, \n",
    "        epochs=2, batch_size=4, vt_batch_size=4, lr=0.0005, lr_decay_factor=0.5, lr_decay_step_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIG-Stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Mar  2 2023, 03:21:46) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7cfb14014bc05b7b9c111b7cecf146c136c646b654c64df2272ca310a47a6635"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
